diff --git a/block/ll_rw_blk.c b/block/ll_rw_blk.c
index 13c40a0..e9d3388 100644
--- a/block/ll_rw_blk.c
+++ b/block/ll_rw_blk.c
@@ -2287,23 +2287,24 @@ int blk_rq_map_user(request_queue_t *q, 
 	 */
 	uaddr = (unsigned long) ubuf;
 	if (!(uaddr & queue_dma_alignment(q)) && !(len & queue_dma_alignment(q)))
-		bio = bio_map_user(q, NULL, uaddr, len, reading, 0);
+		bio = bio_map_user(q, NULL, uaddr, len, reading);
 	else
 		bio = bio_copy_user(q, uaddr, len, reading);
 
-	if (!IS_ERR(bio)) {
-		rq->bio = rq->biotail = bio;
-		blk_rq_bio_prep(q, rq, bio);
+	if (IS_ERR(bio))
+		return PTR_ERR(bio);
 
-		rq->buffer = rq->data = NULL;
-		rq->data_len = len;
-		return 0;
+	if (bio->bi_size != len) {
+		bio_endio(bio, bio->bi_size, 0);
+		bio_unmap_user(bio);
+		return -EINVAL;
 	}
 
-	/*
-	 * bio is the err-ptr
-	 */
-	return PTR_ERR(bio);
+	rq->bio = rq->biotail = bio;
+	blk_rq_bio_prep(q, rq, bio);
+	rq->buffer = rq->data = NULL;
+	rq->data_len = len;
+	return 0;
 }
 
 EXPORT_SYMBOL(blk_rq_map_user);
@@ -2329,7 +2330,7 @@ EXPORT_SYMBOL(blk_rq_map_user);
  *    unmapping.
  */
 int blk_rq_map_user_iov(request_queue_t *q, struct request *rq,
-			struct sg_iovec *iov, int iov_count)
+			struct sg_iovec *iov, int iov_count, unsigned int len)
 {
 	struct bio *bio;
 
@@ -2339,11 +2340,16 @@ int blk_rq_map_user_iov(request_queue_t 
 	/* we don't allow misaligned data like bio_map_user() does.  If the
 	 * user is using sg, they're expected to know the alignment constraints
 	 * and respect them accordingly */
-	bio = bio_map_user_iov(q, NULL, iov, iov_count, rq_data_dir(rq)== READ,
-				0);
+	bio = bio_map_user_iov(q, NULL, iov, iov_count, rq_data_dir(rq)== READ);
 	if (IS_ERR(bio))
 		return PTR_ERR(bio);
 
+	if (bio->bi_size != len) {
+		bio_endio(bio, bio->bi_size, 0);
+		bio_unmap_user(bio);
+		return -EINVAL;
+	}
+
 	rq->bio = rq->biotail = bio;
 	blk_rq_bio_prep(q, rq, bio);
 	rq->buffer = rq->data = NULL;
@@ -2765,16 +2771,12 @@ static void init_request_from_bio(struct
 
 	req->errors = 0;
 	req->hard_sector = req->sector = bio->bi_sector;
-	req->hard_nr_sectors = req->nr_sectors = bio_sectors(bio);
-	req->current_nr_sectors = req->hard_cur_sectors = bio_cur_sectors(bio);
-	req->nr_phys_segments = bio_phys_segments(req->q, bio);
-	req->nr_hw_segments = bio_hw_segments(req->q, bio);
-	req->buffer = bio_data(bio);	/* see ->buffer comment above */
 	req->waiting = NULL;
-	req->bio = req->biotail = bio;
 	req->ioprio = bio_prio(bio);
 	req->rq_disk = bio->bi_bdev->bd_disk;
 	req->start_time = jiffies;
+
+	blk_rq_bio_prep(req->q, req, bio);
 }
 
 static int __make_request(request_queue_t *q, struct bio *bio)
@@ -3403,9 +3405,6 @@ EXPORT_SYMBOL(end_request);
 
 void blk_rq_bio_prep(request_queue_t *q, struct request *rq, struct bio *bio)
 {
-	/* first three bits are identical in rq->flags and bio->bi_rw */
-	rq->flags |= (bio->bi_rw & 7);
-
 	rq->nr_phys_segments = bio_phys_segments(q, bio);
 	rq->nr_hw_segments = bio_hw_segments(q, bio);
 	rq->current_nr_sectors = bio_cur_sectors(bio);
diff --git a/block/scsi_ioctl.c b/block/scsi_ioctl.c
index 24f7af9..ef9900d 100644
--- a/block/scsi_ioctl.c
+++ b/block/scsi_ioctl.c
@@ -274,7 +274,8 @@ static int sg_io(struct file *file, requ
 			goto out;
 		}
 
-		ret = blk_rq_map_user_iov(q, rq, iov, hdr->iovec_count);
+		ret = blk_rq_map_user_iov(q, rq, iov, hdr->iovec_count,
+					  hdr->dxfer_len);
 		kfree(iov);
 	} else if (hdr->dxfer_len)
 		ret = blk_rq_map_user(q, rq, hdr->dxferp, hdr->dxfer_len);
diff --git a/drivers/scsi/ibmvscsi/ibmvscsi.c b/drivers/scsi/ibmvscsi/ibmvscsi.c
index eaefedd..e7bd028 100644
--- a/drivers/scsi/ibmvscsi/ibmvscsi.c
+++ b/drivers/scsi/ibmvscsi/ibmvscsi.c
@@ -168,7 +168,7 @@ static void release_event_pool(struct ev
 			++in_use;
 		if (pool->events[i].ext_list) {
 			dma_free_coherent(hostdata->dev,
-				  SG_ALL * sizeof(struct memory_descriptor),
+				  SG_ALL * sizeof(struct srp_direct_buf),
 				  pool->events[i].ext_list,
 				  pool->events[i].ext_list_token);
 		}
@@ -284,40 +284,37 @@ static void set_srp_direction(struct scs
 			      struct srp_cmd *srp_cmd, 
 			      int numbuf)
 {
+	u8 fmt;
+
 	if (numbuf == 0)
 		return;
 	
-	if (numbuf == 1) {
+	if (numbuf == 1)
+		fmt = SRP_DATA_DESC_DIRECT;
+	else {
+		fmt = SRP_DATA_DESC_INDIRECT;
+		numbuf = min(numbuf, MAX_INDIRECT_BUFS);
+
 		if (cmd->sc_data_direction == DMA_TO_DEVICE)
-			srp_cmd->data_out_format = SRP_DIRECT_BUFFER;
-		else 
-			srp_cmd->data_in_format = SRP_DIRECT_BUFFER;
-	} else {
-		if (cmd->sc_data_direction == DMA_TO_DEVICE) {
-			srp_cmd->data_out_format = SRP_INDIRECT_BUFFER;
-			srp_cmd->data_out_count =
-				numbuf < MAX_INDIRECT_BUFS ?
-					numbuf: MAX_INDIRECT_BUFS;
-		} else {
-			srp_cmd->data_in_format = SRP_INDIRECT_BUFFER;
-			srp_cmd->data_in_count =
-				numbuf < MAX_INDIRECT_BUFS ?
-					numbuf: MAX_INDIRECT_BUFS;
-		}
+			srp_cmd->data_out_desc_cnt = numbuf;
+		else
+			srp_cmd->data_in_desc_cnt = numbuf;
 	}
+
+	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+		srp_cmd->buf_fmt = fmt << 4;
+	else
+		srp_cmd->buf_fmt = fmt;
 }
 
-static void unmap_sg_list(int num_entries, 
+static void unmap_sg_list(int num_entries,
 		struct device *dev,
-		struct memory_descriptor *md)
-{ 
+		struct srp_direct_buf *md)
+{
 	int i;
 
-	for (i = 0; i < num_entries; ++i) {
-		dma_unmap_single(dev,
-			md[i].virtual_address,
-			md[i].length, DMA_BIDIRECTIONAL);
-	}
+	for (i = 0; i < num_entries; ++i)
+		dma_unmap_single(dev, md[i].va, md[i].len, DMA_BIDIRECTIONAL);
 }
 
 /**
@@ -330,23 +327,26 @@ static void unmap_cmd_data(struct srp_cm
 			   struct srp_event_struct *evt_struct,
 			   struct device *dev)
 {
-	if ((cmd->data_out_format == SRP_NO_BUFFER) &&
-	    (cmd->data_in_format == SRP_NO_BUFFER))
+	u8 out_fmt, in_fmt;
+
+	out_fmt = cmd->buf_fmt >> 4;
+	in_fmt = cmd->buf_fmt & ((1U << 4) - 1);
+
+	if (out_fmt == SRP_NO_DATA_DESC && in_fmt == SRP_NO_DATA_DESC)
 		return;
-	else if ((cmd->data_out_format == SRP_DIRECT_BUFFER) ||
-		 (cmd->data_in_format == SRP_DIRECT_BUFFER)) {
-		struct memory_descriptor *data =
-			(struct memory_descriptor *)cmd->additional_data;
-		dma_unmap_single(dev, data->virtual_address, data->length,
-				 DMA_BIDIRECTIONAL);
+	else if (out_fmt == SRP_DATA_DESC_DIRECT ||
+		 in_fmt == SRP_DATA_DESC_DIRECT) {
+		struct srp_direct_buf *data =
+			(struct srp_direct_buf *) cmd->add_data;
+		dma_unmap_single(dev, data->va, data->len, DMA_BIDIRECTIONAL);
 	} else {
-		struct indirect_descriptor *indirect =
-			(struct indirect_descriptor *)cmd->additional_data;
-		int num_mapped = indirect->head.length / 
-			sizeof(indirect->list[0]);
+		struct srp_indirect_buf *indirect =
+			(struct srp_indirect_buf *) cmd->add_data;
+		int num_mapped = indirect->table_desc.len /
+			sizeof(struct srp_direct_buf);
 
 		if (num_mapped <= MAX_INDIRECT_BUFS) {
-			unmap_sg_list(num_mapped, dev, &indirect->list[0]);
+			unmap_sg_list(num_mapped, dev, &indirect->desc_list[0]);
 			return;
 		}
 
@@ -356,17 +356,17 @@ static void unmap_cmd_data(struct srp_cm
 
 static int map_sg_list(int num_entries, 
 		       struct scatterlist *sg,
-		       struct memory_descriptor *md)
+		       struct srp_direct_buf *md)
 {
 	int i;
 	u64 total_length = 0;
 
 	for (i = 0; i < num_entries; ++i) {
-		struct memory_descriptor *descr = md + i;
+		struct srp_direct_buf *descr = md + i;
 		struct scatterlist *sg_entry = &sg[i];
-		descr->virtual_address = sg_dma_address(sg_entry);
-		descr->length = sg_dma_len(sg_entry);
-		descr->memory_handle = 0;
+		descr->va = sg_dma_address(sg_entry);
+		descr->len = sg_dma_len(sg_entry);
+		descr->key = 0;
 		total_length += sg_dma_len(sg_entry);
  	}
 	return total_length;
@@ -389,10 +389,10 @@ static int map_sg_data(struct scsi_cmnd 
 	int sg_mapped;
 	u64 total_length = 0;
 	struct scatterlist *sg = cmd->request_buffer;
-	struct memory_descriptor *data =
-	    (struct memory_descriptor *)srp_cmd->additional_data;
-	struct indirect_descriptor *indirect =
-	    (struct indirect_descriptor *)data;
+	struct srp_direct_buf *data =
+		(struct srp_direct_buf *) srp_cmd->add_data;
+	struct srp_indirect_buf *indirect =
+		(struct srp_indirect_buf *) data;
 
 	sg_mapped = dma_map_sg(dev, sg, cmd->use_sg, DMA_BIDIRECTIONAL);
 
@@ -403,9 +403,9 @@ static int map_sg_data(struct scsi_cmnd 
 
 	/* special case; we can use a single direct descriptor */
 	if (sg_mapped == 1) {
-		data->virtual_address = sg_dma_address(&sg[0]);
-		data->length = sg_dma_len(&sg[0]);
-		data->memory_handle = 0;
+		data->va = sg_dma_address(&sg[0]);
+		data->len = sg_dma_len(&sg[0]);
+		data->key = 0;
 		return 1;
 	}
 
@@ -416,25 +416,26 @@ static int map_sg_data(struct scsi_cmnd 
 		return 0;
 	}
 
-	indirect->head.virtual_address = 0;
-	indirect->head.length = sg_mapped * sizeof(indirect->list[0]);
-	indirect->head.memory_handle = 0;
+	indirect->table_desc.va = 0;
+	indirect->table_desc.len = sg_mapped * sizeof(struct srp_direct_buf);
+	indirect->table_desc.key = 0;
 
 	if (sg_mapped <= MAX_INDIRECT_BUFS) {
-		total_length = map_sg_list(sg_mapped, sg, &indirect->list[0]);
-		indirect->total_length = total_length;
+		total_length = map_sg_list(sg_mapped, sg,
+					   &indirect->desc_list[0]);
+		indirect->len = total_length;
 		return 1;
 	}
 
 	/* get indirect table */
 	if (!evt_struct->ext_list) {
-		evt_struct->ext_list =(struct memory_descriptor*)
+		evt_struct->ext_list = (struct srp_direct_buf *)
 			dma_alloc_coherent(dev, 
-				SG_ALL * sizeof(struct memory_descriptor),
-				&evt_struct->ext_list_token, 0);
+					   SG_ALL * sizeof(struct srp_direct_buf),
+					   &evt_struct->ext_list_token, 0);
 		if (!evt_struct->ext_list) {
-		    printk(KERN_ERR
-		   	"ibmvscsi: Can't allocate memory for indirect table\n");
+			printk(KERN_ERR
+			       "ibmvscsi: Can't allocate memory for indirect table\n");
 			return 0;
 			
 		}
@@ -442,11 +443,11 @@ static int map_sg_data(struct scsi_cmnd 
 
 	total_length = map_sg_list(sg_mapped, sg, evt_struct->ext_list);	
 
-	indirect->total_length = total_length;
-	indirect->head.virtual_address = evt_struct->ext_list_token;
-	indirect->head.length = sg_mapped * sizeof(indirect->list[0]);
-	memcpy(indirect->list, evt_struct->ext_list,
-		MAX_INDIRECT_BUFS * sizeof(struct memory_descriptor));
+	indirect->len = total_length;
+	indirect->table_desc.va = evt_struct->ext_list_token;
+	indirect->table_desc.len = sg_mapped * sizeof(indirect->desc_list[0]);
+	memcpy(indirect->desc_list, evt_struct->ext_list,
+	       MAX_INDIRECT_BUFS * sizeof(struct srp_direct_buf));
 	
  	return 1;
 }
@@ -463,20 +464,20 @@ static int map_sg_data(struct scsi_cmnd 
 static int map_single_data(struct scsi_cmnd *cmd,
 			   struct srp_cmd *srp_cmd, struct device *dev)
 {
-	struct memory_descriptor *data =
-	    (struct memory_descriptor *)srp_cmd->additional_data;
+	struct srp_direct_buf *data =
+		(struct srp_direct_buf *) srp_cmd->add_data;
 
-	data->virtual_address =
+	data->va =
 		dma_map_single(dev, cmd->request_buffer,
 			       cmd->request_bufflen,
 			       DMA_BIDIRECTIONAL);
-	if (dma_mapping_error(data->virtual_address)) {
+	if (dma_mapping_error(data->va)) {
 		printk(KERN_ERR
 		       "ibmvscsi: Unable to map request_buffer for command!\n");
 		return 0;
 	}
-	data->length = cmd->request_bufflen;
-	data->memory_handle = 0;
+	data->len = cmd->request_bufflen;
+	data->key = 0;
 
 	set_srp_direction(cmd, srp_cmd, 1);
 
@@ -548,7 +549,7 @@ static int ibmvscsi_send_srp_event(struc
 
 	/* Copy the IU into the transfer area */
 	*evt_struct->xfer_iu = evt_struct->iu;
-	evt_struct->xfer_iu->srp.generic.tag = (u64)evt_struct;
+	evt_struct->xfer_iu->srp.rsp.tag = (u64)evt_struct;
 
 	/* Add this to the sent list.  We need to do this 
 	 * before we actually send 
@@ -586,27 +587,27 @@ static void handle_cmd_rsp(struct srp_ev
 	struct srp_rsp *rsp = &evt_struct->xfer_iu->srp.rsp;
 	struct scsi_cmnd *cmnd = evt_struct->cmnd;
 
-	if (unlikely(rsp->type != SRP_RSP_TYPE)) {
+	if (unlikely(rsp->opcode != SRP_RSP)) {
 		if (printk_ratelimit())
 			printk(KERN_WARNING 
 			       "ibmvscsi: bad SRP RSP type %d\n",
-			       rsp->type);
+			       rsp->opcode);
 	}
 	
 	if (cmnd) {
 		cmnd->result = rsp->status;
 		if (((cmnd->result >> 1) & 0x1f) == CHECK_CONDITION)
 			memcpy(cmnd->sense_buffer,
-			       rsp->sense_and_response_data,
-			       rsp->sense_data_list_length);
+			       rsp->data,
+			       rsp->sense_data_len);
 		unmap_cmd_data(&evt_struct->iu.srp.cmd, 
 			       evt_struct, 
 			       evt_struct->hostdata->dev);
 
-		if (rsp->doover)
-			cmnd->resid = rsp->data_out_residual_count;
-		else if (rsp->diover)
-			cmnd->resid = rsp->data_in_residual_count;
+		if (rsp->flags & SRP_RSP_FLAG_DOOVER)
+			cmnd->resid = rsp->data_out_res_cnt;
+		else if (rsp->flags & SRP_RSP_FLAG_DIOVER)
+			cmnd->resid = rsp->data_in_res_cnt;
 	}
 
 	if (evt_struct->cmnd_done)
@@ -633,10 +634,11 @@ static int ibmvscsi_queuecommand(struct 
 {
 	struct srp_cmd *srp_cmd;
 	struct srp_event_struct *evt_struct;
-	struct indirect_descriptor *indirect;
+	struct srp_indirect_buf *indirect;
 	struct ibmvscsi_host_data *hostdata =
 		(struct ibmvscsi_host_data *)&cmnd->device->host->hostdata;
 	u16 lun = lun_from_dev(cmnd->device);
+	u8 out_fmt, in_fmt;
 
 	evt_struct = get_event_struct(&hostdata->pool);
 	if (!evt_struct)
@@ -644,8 +646,8 @@ static int ibmvscsi_queuecommand(struct 
 
 	/* Set up the actual SRP IU */
 	srp_cmd = &evt_struct->iu.srp.cmd;
-	memset(srp_cmd, 0x00, sizeof(*srp_cmd));
-	srp_cmd->type = SRP_CMD_TYPE;
+	memset(srp_cmd, 0x00, SRP_MAX_IU_LEN);
+	srp_cmd->opcode = SRP_CMD;
 	memcpy(srp_cmd->cdb, cmnd->cmnd, sizeof(cmnd->cmnd));
 	srp_cmd->lun = ((u64) lun) << 48;
 
@@ -664,13 +666,15 @@ static int ibmvscsi_queuecommand(struct 
 	evt_struct->cmnd_done = done;
 
 	/* Fix up dma address of the buffer itself */
-	indirect = (struct indirect_descriptor *)srp_cmd->additional_data;
-	if (((srp_cmd->data_out_format == SRP_INDIRECT_BUFFER) ||
-	    (srp_cmd->data_in_format == SRP_INDIRECT_BUFFER)) &&
-	    (indirect->head.virtual_address == 0)) {
-		indirect->head.virtual_address = evt_struct->crq.IU_data_ptr +
-		    offsetof(struct srp_cmd, additional_data) +
-		    offsetof(struct indirect_descriptor, list);
+	indirect = (struct srp_indirect_buf *) srp_cmd->add_data;
+	out_fmt = srp_cmd->buf_fmt >> 4;
+	in_fmt = srp_cmd->buf_fmt & ((1U << 4) - 1);
+	if ((in_fmt == SRP_DATA_DESC_INDIRECT ||
+	     out_fmt == SRP_DATA_DESC_INDIRECT) &&
+	    indirect->table_desc.va == 0) {
+		indirect->table_desc.va = evt_struct->crq.IU_data_ptr +
+			offsetof(struct srp_cmd, add_data) +
+			offsetof(struct srp_indirect_buf, desc_list);
 	}
 
 	return ibmvscsi_send_srp_event(evt_struct, hostdata);
@@ -780,10 +784,10 @@ static void send_mad_adapter_info(struct
 static void login_rsp(struct srp_event_struct *evt_struct)
 {
 	struct ibmvscsi_host_data *hostdata = evt_struct->hostdata;
-	switch (evt_struct->xfer_iu->srp.generic.type) {
-	case SRP_LOGIN_RSP_TYPE:	/* it worked! */
+	switch (evt_struct->xfer_iu->srp.login_rsp.opcode) {
+	case SRP_LOGIN_RSP:	/* it worked! */
 		break;
-	case SRP_LOGIN_REJ_TYPE:	/* refused! */
+	case SRP_LOGIN_REJ:	/* refused! */
 		printk(KERN_INFO "ibmvscsi: SRP_LOGIN_REJ reason %u\n",
 		       evt_struct->xfer_iu->srp.login_rej.reason);
 		/* Login failed.  */
@@ -792,7 +796,7 @@ static void login_rsp(struct srp_event_s
 	default:
 		printk(KERN_ERR
 		       "ibmvscsi: Invalid login response typecode 0x%02x!\n",
-		       evt_struct->xfer_iu->srp.generic.type);
+		       evt_struct->xfer_iu->srp.login_rsp.opcode);
 		/* Login failed.  */
 		atomic_set(&hostdata->request_limit, -1);
 		return;
@@ -800,17 +804,17 @@ static void login_rsp(struct srp_event_s
 
 	printk(KERN_INFO "ibmvscsi: SRP_LOGIN succeeded\n");
 
-	if (evt_struct->xfer_iu->srp.login_rsp.request_limit_delta >
+	if (evt_struct->xfer_iu->srp.login_rsp.req_lim_delta >
 	    (max_requests - 2))
-		evt_struct->xfer_iu->srp.login_rsp.request_limit_delta =
+		evt_struct->xfer_iu->srp.login_rsp.req_lim_delta =
 		    max_requests - 2;
 
 	/* Now we know what the real request-limit is */
 	atomic_set(&hostdata->request_limit,
-		   evt_struct->xfer_iu->srp.login_rsp.request_limit_delta);
+		   evt_struct->xfer_iu->srp.login_rsp.req_lim_delta);
 
 	hostdata->host->can_queue =
-	    evt_struct->xfer_iu->srp.login_rsp.request_limit_delta - 2;
+	    evt_struct->xfer_iu->srp.login_rsp.req_lim_delta - 2;
 
 	if (hostdata->host->can_queue < 1) {
 		printk(KERN_ERR "ibmvscsi: Invalid request_limit_delta\n");
@@ -849,9 +853,9 @@ static int send_srp_login(struct ibmvscs
 
 	login = &evt_struct->iu.srp.login_req;
 	memset(login, 0x00, sizeof(struct srp_login_req));
-	login->type = SRP_LOGIN_REQ_TYPE;
-	login->max_requested_initiator_to_target_iulen = sizeof(union srp_iu);
-	login->required_buffer_formats = 0x0006;
+	login->opcode = SRP_LOGIN_REQ;
+	login->req_it_iu_len = sizeof(union srp_iu);
+	login->req_buf_fmt = SRP_BUF_FORMAT_DIRECT | SRP_BUF_FORMAT_INDIRECT;
 	
 	/* Start out with a request limit of 1, since this is negotiated in
 	 * the login request we are just sending
@@ -928,13 +932,13 @@ static int ibmvscsi_eh_abort_handler(str
 	
 	/* Set up an abort SRP command */
 	memset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));
-	tsk_mgmt->type = SRP_TSK_MGMT_TYPE;
+	tsk_mgmt->opcode = SRP_TSK_MGMT;
 	tsk_mgmt->lun = ((u64) lun) << 48;
-	tsk_mgmt->task_mgmt_flags = 0x01;	/* ABORT TASK */
-	tsk_mgmt->managed_task_tag = (u64) found_evt;
+	tsk_mgmt->tsk_mgmt_func = SRP_TSK_ABORT_TASK;
+	tsk_mgmt->task_tag = (u64) found_evt;
 
 	printk(KERN_INFO "ibmvscsi: aborting command. lun 0x%lx, tag 0x%lx\n",
-	       tsk_mgmt->lun, tsk_mgmt->managed_task_tag);
+	       tsk_mgmt->lun, tsk_mgmt->task_tag);
 
 	evt->sync_srp = &srp_rsp;
 	init_completion(&evt->comp);
@@ -948,25 +952,25 @@ static int ibmvscsi_eh_abort_handler(str
 	wait_for_completion(&evt->comp);
 
 	/* make sure we got a good response */
-	if (unlikely(srp_rsp.srp.generic.type != SRP_RSP_TYPE)) {
+	if (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {
 		if (printk_ratelimit())
 			printk(KERN_WARNING 
 			       "ibmvscsi: abort bad SRP RSP type %d\n",
-			       srp_rsp.srp.generic.type);
+			       srp_rsp.srp.rsp.opcode);
 		return FAILED;
 	}
 
-	if (srp_rsp.srp.rsp.rspvalid)
-		rsp_rc = *((int *)srp_rsp.srp.rsp.sense_and_response_data);
+	if (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)
+		rsp_rc = *((int *)srp_rsp.srp.rsp.data);
 	else
 		rsp_rc = srp_rsp.srp.rsp.status;
 
 	if (rsp_rc) {
 		if (printk_ratelimit())
 			printk(KERN_WARNING 
-		       "ibmvscsi: abort code %d for task tag 0x%lx\n",
+			       "ibmvscsi: abort code %d for task tag 0x%lx\n",
 			       rsp_rc,
-			       tsk_mgmt->managed_task_tag);
+			       tsk_mgmt->task_tag);
 		return FAILED;
 	}
 
@@ -987,13 +991,13 @@ static int ibmvscsi_eh_abort_handler(str
 		spin_unlock_irqrestore(hostdata->host->host_lock, flags);
 		printk(KERN_INFO
 		       "ibmvscsi: aborted task tag 0x%lx completed\n",
-		       tsk_mgmt->managed_task_tag);
+		       tsk_mgmt->task_tag);
 		return SUCCESS;
 	}
 
 	printk(KERN_INFO
 	       "ibmvscsi: successfully aborted task tag 0x%lx\n",
-	       tsk_mgmt->managed_task_tag);
+	       tsk_mgmt->task_tag);
 
 	cmd->result = (DID_ABORT << 16);
 	list_del(&found_evt->list);
@@ -1040,9 +1044,9 @@ static int ibmvscsi_eh_device_reset_hand
 
 	/* Set up a lun reset SRP command */
 	memset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));
-	tsk_mgmt->type = SRP_TSK_MGMT_TYPE;
+	tsk_mgmt->opcode = SRP_TSK_MGMT;
 	tsk_mgmt->lun = ((u64) lun) << 48;
-	tsk_mgmt->task_mgmt_flags = 0x08;	/* LUN RESET */
+	tsk_mgmt->tsk_mgmt_func = SRP_TSK_LUN_RESET;
 
 	printk(KERN_INFO "ibmvscsi: resetting device. lun 0x%lx\n",
 	       tsk_mgmt->lun);
@@ -1059,16 +1063,16 @@ static int ibmvscsi_eh_device_reset_hand
 	wait_for_completion(&evt->comp);
 
 	/* make sure we got a good response */
-	if (unlikely(srp_rsp.srp.generic.type != SRP_RSP_TYPE)) {
+	if (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {
 		if (printk_ratelimit())
 			printk(KERN_WARNING 
 			       "ibmvscsi: reset bad SRP RSP type %d\n",
-			       srp_rsp.srp.generic.type);
+			       srp_rsp.srp.rsp.opcode);
 		return FAILED;
 	}
 
-	if (srp_rsp.srp.rsp.rspvalid)
-		rsp_rc = *((int *)srp_rsp.srp.rsp.sense_and_response_data);
+	if (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)
+		rsp_rc = *((int *)srp_rsp.srp.rsp.data);
 	else
 		rsp_rc = srp_rsp.srp.rsp.status;
 
@@ -1076,8 +1080,7 @@ static int ibmvscsi_eh_device_reset_hand
 		if (printk_ratelimit())
 			printk(KERN_WARNING 
 			       "ibmvscsi: reset code %d for task tag 0x%lx\n",
-		       rsp_rc,
-			       tsk_mgmt->managed_task_tag);
+			       rsp_rc, tsk_mgmt->task_tag);
 		return FAILED;
 	}
 
@@ -1226,7 +1229,7 @@ void ibmvscsi_handle_crq(struct viosrp_c
 	}
 
 	if (crq->format == VIOSRP_SRP_FORMAT)
-		atomic_add(evt_struct->xfer_iu->srp.rsp.request_limit_delta,
+		atomic_add(evt_struct->xfer_iu->srp.rsp.req_lim_delta,
 			   &hostdata->request_limit);
 
 	if (evt_struct->done)
diff --git a/drivers/scsi/ibmvscsi/ibmvscsi.h b/drivers/scsi/ibmvscsi/ibmvscsi.h
index 4550d71..5c6d935 100644
--- a/drivers/scsi/ibmvscsi/ibmvscsi.h
+++ b/drivers/scsi/ibmvscsi/ibmvscsi.h
@@ -68,7 +68,7 @@ struct srp_event_struct {
 	void (*cmnd_done) (struct scsi_cmnd *);
 	struct completion comp;
 	union viosrp_iu *sync_srp;
-	struct memory_descriptor *ext_list;
+	struct srp_direct_buf *ext_list;
 	dma_addr_t ext_list_token;
 };
 
diff --git a/drivers/scsi/ibmvscsi/rpa_vscsi.c b/drivers/scsi/ibmvscsi/rpa_vscsi.c
index f47dd87..58aa530 100644
--- a/drivers/scsi/ibmvscsi/rpa_vscsi.c
+++ b/drivers/scsi/ibmvscsi/rpa_vscsi.c
@@ -34,7 +34,6 @@
 #include <linux/dma-mapping.h>
 #include <linux/interrupt.h>
 #include "ibmvscsi.h"
-#include "srp.h"
 
 static char partition_name[97] = "UNKNOWN";
 static unsigned int partition_number = -1;
diff --git a/drivers/scsi/ibmvscsi/srp.h b/drivers/scsi/ibmvscsi/srp.h
deleted file mode 100644
index 7d8e4c4..0000000
--- a/drivers/scsi/ibmvscsi/srp.h
+++ /dev/null
@@ -1,227 +0,0 @@
-/*****************************************************************************/
-/* srp.h -- SCSI RDMA Protocol definitions                                   */
-/*                                                                           */
-/* Written By: Colin Devilbis, IBM Corporation                               */
-/*                                                                           */
-/* Copyright (C) 2003 IBM Corporation                                        */
-/*                                                                           */
-/* This program is free software; you can redistribute it and/or modify      */
-/* it under the terms of the GNU General Public License as published by      */
-/* the Free Software Foundation; either version 2 of the License, or         */
-/* (at your option) any later version.                                       */
-/*                                                                           */
-/* This program is distributed in the hope that it will be useful,           */
-/* but WITHOUT ANY WARRANTY; without even the implied warranty of            */
-/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             */
-/* GNU General Public License for more details.                              */
-/*                                                                           */
-/* You should have received a copy of the GNU General Public License         */
-/* along with this program; if not, write to the Free Software               */
-/* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */
-/*                                                                           */
-/*                                                                           */
-/* This file contains structures and definitions for the SCSI RDMA Protocol  */
-/* (SRP) as defined in the T10 standard available at www.t10.org.  This      */
-/* file was based on the 16a version of the standard                         */
-/*                                                                           */
-/*****************************************************************************/
-#ifndef SRP_H
-#define SRP_H
-
-#define SRP_VERSION "16.a"
-
-#define PACKED __attribute__((packed))
-
-enum srp_types {
-	SRP_LOGIN_REQ_TYPE = 0x00,
-	SRP_LOGIN_RSP_TYPE = 0xC0,
-	SRP_LOGIN_REJ_TYPE = 0xC2,
-	SRP_I_LOGOUT_TYPE = 0x03,
-	SRP_T_LOGOUT_TYPE = 0x80,
-	SRP_TSK_MGMT_TYPE = 0x01,
-	SRP_CMD_TYPE = 0x02,
-	SRP_RSP_TYPE = 0xC1,
-	SRP_CRED_REQ_TYPE = 0x81,
-	SRP_CRED_RSP_TYPE = 0x41,
-	SRP_AER_REQ_TYPE = 0x82,
-	SRP_AER_RSP_TYPE = 0x42
-};
-
-enum srp_descriptor_formats {
-	SRP_NO_BUFFER = 0x00,
-	SRP_DIRECT_BUFFER = 0x01,
-	SRP_INDIRECT_BUFFER = 0x02
-};
-
-struct memory_descriptor {
-	u64 virtual_address;
-	u32 memory_handle;
-	u32 length;
-};
-
-struct indirect_descriptor {
-	struct memory_descriptor head;
-	u32 total_length;
-	struct memory_descriptor list[1] PACKED;
-};
-
-struct srp_generic {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-};
-
-struct srp_login_req {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-	u32 max_requested_initiator_to_target_iulen;
-	u32 reserved2;
-	u16 required_buffer_formats;
-	u8 reserved3:6;
-	u8 multi_channel_action:2;
-	u8 reserved4;
-	u32 reserved5;
-	u8 initiator_port_identifier[16];
-	u8 target_port_identifier[16];
-};
-
-struct srp_login_rsp {
-	u8 type;
-	u8 reserved1[3];
-	u32 request_limit_delta;
-	u64 tag;
-	u32 max_initiator_to_target_iulen;
-	u32 max_target_to_initiator_iulen;
-	u16 supported_buffer_formats;
-	u8 reserved2:6;
-	u8 multi_channel_result:2;
-	u8 reserved3;
-	u8 reserved4[24];
-};
-
-struct srp_login_rej {
-	u8 type;
-	u8 reserved1[3];
-	u32 reason;
-	u64 tag;
-	u64 reserved2;
-	u16 supported_buffer_formats;
-	u8 reserved3[6];
-};
-
-struct srp_i_logout {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-};
-
-struct srp_t_logout {
-	u8 type;
-	u8 reserved1[3];
-	u32 reason;
-	u64 tag;
-};
-
-struct srp_tsk_mgmt {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-	u32 reserved2;
-	u64 lun PACKED;
-	u8 reserved3;
-	u8 reserved4;
-	u8 task_mgmt_flags;
-	u8 reserved5;
-	u64 managed_task_tag;
-	u64 reserved6;
-};
-
-struct srp_cmd {
-	u8 type;
-	u32 reserved1 PACKED;
-	u8 data_out_format:4;
-	u8 data_in_format:4;
-	u8 data_out_count;
-	u8 data_in_count;
-	u64 tag;
-	u32 reserved2;
-	u64 lun PACKED;
-	u8 reserved3;
-	u8 reserved4:5;
-	u8 task_attribute:3;
-	u8 reserved5;
-	u8 additional_cdb_len;
-	u8 cdb[16];
-	u8 additional_data[0x100 - 0x30];
-};
-
-struct srp_rsp {
-	u8 type;
-	u8 reserved1[3];
-	u32 request_limit_delta;
-	u64 tag;
-	u16 reserved2;
-	u8 reserved3:2;
-	u8 diunder:1;
-	u8 diover:1;
-	u8 dounder:1;
-	u8 doover:1;
-	u8 snsvalid:1;
-	u8 rspvalid:1;
-	u8 status;
-	u32 data_in_residual_count;
-	u32 data_out_residual_count;
-	u32 sense_data_list_length;
-	u32 response_data_list_length;
-	u8 sense_and_response_data[18];
-};
-
-struct srp_cred_req {
-	u8 type;
-	u8 reserved1[3];
-	u32 request_limit_delta;
-	u64 tag;
-};
-
-struct srp_cred_rsp {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-};
-
-struct srp_aer_req {
-	u8 type;
-	u8 reserved1[3];
-	u32 request_limit_delta;
-	u64 tag;
-	u32 reserved2;
-	u64 lun;
-	u32 sense_data_list_length;
-	u32 reserved3;
-	u8 sense_data[20];
-};
-
-struct srp_aer_rsp {
-	u8 type;
-	u8 reserved1[7];
-	u64 tag;
-};
-
-union srp_iu {
-	struct srp_generic generic;
-	struct srp_login_req login_req;
-	struct srp_login_rsp login_rsp;
-	struct srp_login_rej login_rej;
-	struct srp_i_logout i_logout;
-	struct srp_t_logout t_logout;
-	struct srp_tsk_mgmt tsk_mgmt;
-	struct srp_cmd cmd;
-	struct srp_rsp rsp;
-	struct srp_cred_req cred_req;
-	struct srp_cred_rsp cred_rsp;
-	struct srp_aer_req aer_req;
-	struct srp_aer_rsp aer_rsp;
-};
-
-#endif
diff --git a/drivers/scsi/ibmvscsi/viosrp.h b/drivers/scsi/ibmvscsi/viosrp.h
index 6a6bba8..90f1a61 100644
--- a/drivers/scsi/ibmvscsi/viosrp.h
+++ b/drivers/scsi/ibmvscsi/viosrp.h
@@ -33,7 +33,22 @@
 /*****************************************************************************/
 #ifndef VIOSRP_H
 #define VIOSRP_H
-#include "srp.h"
+#include <scsi/srp.h>
+
+#define SRP_VERSION "16.a"
+#define SRP_MAX_IU_LEN	256
+
+union srp_iu {
+	struct srp_login_req login_req;
+	struct srp_login_rsp login_rsp;
+	struct srp_login_rej login_rej;
+	struct srp_i_logout i_logout;
+	struct srp_t_logout t_logout;
+	struct srp_tsk_mgmt tsk_mgmt;
+	struct srp_cmd cmd;
+	struct srp_rsp rsp;
+	u8 reserved[SRP_MAX_IU_LEN];
+};
 
 enum viosrp_crq_formats {
 	VIOSRP_SRP_FORMAT = 0x01,
diff --git a/drivers/scsi/scsi.c b/drivers/scsi/scsi.c
index 3cf02b1..9c22465 100644
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -352,8 +352,6 @@ void scsi_host_put_command(struct Scsi_H
 	spin_unlock(&shost->free_list_lock);
 
 	spin_lock(q->queue_lock);
-	if (blk_rq_tagged(rq))
-		blk_queue_end_tag(q, rq);
 	__blk_put_request(q, rq);
 	spin_unlock_irqrestore(q->queue_lock, flags);
 
diff --git a/drivers/scsi/scsi_tgt_if.c b/drivers/scsi/scsi_tgt_if.c
index 38b35da..ba1b75b 100644
--- a/drivers/scsi/scsi_tgt_if.c
+++ b/drivers/scsi/scsi_tgt_if.c
@@ -35,15 +35,15 @@
 static int tgtd_pid;
 static struct sock *nl_sk;
 
-static int send_event_res(uint16_t type, struct tgt_event *p,
-			  void *data, int dlen, gfp_t flags, pid_t pid)
+static int send_event_rsp(uint16_t type, struct tgt_event *p, gfp_t flags,
+			  pid_t pid)
 {
 	struct tgt_event *ev;
 	struct nlmsghdr *nlh;
 	struct sk_buff *skb;
 	uint32_t len;
 
-	len = NLMSG_SPACE(sizeof(*ev) + dlen);
+	len = NLMSG_SPACE(sizeof(*ev));
 	skb = alloc_skb(len, flags);
 	if (!skb)
 		return -ENOMEM;
@@ -52,26 +52,27 @@ static int send_event_res(uint16_t type,
 
 	ev = NLMSG_DATA(nlh);
 	memcpy(ev, p, sizeof(*ev));
-	if (dlen)
-		memcpy(ev->data, data, dlen);
 
 	return netlink_unicast(nl_sk, skb, pid, 0);
 }
 
-int scsi_tgt_uspace_send(struct scsi_cmnd *cmd, struct scsi_lun *lun, gfp_t gfp_mask)
+int scsi_tgt_uspace_send(struct scsi_cmnd *cmd, struct scsi_lun *lun, u64 tag,
+			 gfp_t flags)
 {
 	struct Scsi_Host *shost = scsi_tgt_cmd_to_host(cmd);
 	struct sk_buff *skb;
 	struct nlmsghdr *nlh;
 	struct tgt_event *ev;
-	struct tgt_cmd *tcmd;
 	int err, len;
 
-	len = NLMSG_SPACE(sizeof(*ev) + sizeof(struct tgt_cmd));
+	/* FIXME: we need scsi core to do that. */
+	memcpy(cmd->cmnd, cmd->data_cmnd, MAX_COMMAND_SIZE);
+
+	len = NLMSG_SPACE(sizeof(*ev));
 	/*
 	 * TODO: add MAX_COMMAND_SIZE to ev and add mempool
 	 */
-	skb = alloc_skb(NLMSG_SPACE(len), gfp_mask);
+	skb = alloc_skb(NLMSG_SPACE(len), flags);
 	if (!skb)
 		return -ENOMEM;
 
@@ -82,16 +83,14 @@ int scsi_tgt_uspace_send(struct scsi_cmn
 	ev->k.cmd_req.host_no = shost->host_no;
 	ev->k.cmd_req.cid = cmd->request->tag;
 	ev->k.cmd_req.data_len = cmd->request_bufflen;
-
-	dprintk("%d %u %u\n", ev->k.cmd_req.host_no, ev->k.cmd_req.cid,
-		ev->k.cmd_req.data_len);
-
-	/* FIXME: we need scsi core to do that. */
-	memcpy(cmd->cmnd, cmd->data_cmnd, MAX_COMMAND_SIZE);
-
-	tcmd = (struct tgt_cmd *) ev->data;
-	memcpy(tcmd->scb, cmd->cmnd, sizeof(tcmd->scb));
-	memcpy(tcmd->lun, lun, sizeof(struct scsi_lun));
+	memcpy(ev->k.cmd_req.scb, cmd->cmnd, sizeof(ev->k.cmd_req.scb));
+	memcpy(ev->k.cmd_req.lun, lun, sizeof(ev->k.cmd_req.lun));
+	ev->k.cmd_req.attribute = cmd->tag;
+	ev->k.cmd_req.tag = tag;
+
+	dprintk("%p %d %u %u %x %llx\n", cmd, shost->host_no, ev->k.cmd_req.cid,
+		ev->k.cmd_req.data_len, cmd->tag,
+		(unsigned long long) ev->k.cmd_req.tag);
 
 	err = netlink_unicast(nl_sk, skb, tgtd_pid, 0);
 	if (err < 0)
@@ -104,15 +103,31 @@ int scsi_tgt_uspace_send_status(struct s
 {
 	struct Scsi_Host *shost = scsi_tgt_cmd_to_host(cmd);
 	struct tgt_event ev;
-	char dummy[sizeof(struct tgt_cmd)];
 
 	memset(&ev, 0, sizeof(ev));
 	ev.k.cmd_done.host_no = shost->host_no;
 	ev.k.cmd_done.cid = cmd->request->tag;
 	ev.k.cmd_done.result = cmd->result;
 
-	return send_event_res(TGT_KEVENT_CMD_DONE, &ev, dummy, sizeof(dummy),
-			      gfp_mask, tgtd_pid);
+	return send_event_rsp(TGT_KEVENT_CMD_DONE, &ev, gfp_mask, tgtd_pid);
+}
+
+int scsi_tgt_uspace_send_tsk_mgmt(int host_no, int function, u64 tag,
+				  struct scsi_lun *scsilun, void *data)
+{
+	struct tgt_event ev;
+
+	memset(&ev, 0, sizeof(ev));
+	ev.k.tsk_mgmt_req.host_no = host_no;
+	ev.k.tsk_mgmt_req.function = function;
+	ev.k.tsk_mgmt_req.tag = tag;
+	memcpy(ev.k.tsk_mgmt_req.lun, scsilun, sizeof(ev.k.tsk_mgmt_req.lun));
+	ev.k.tsk_mgmt_req.mid = (u64) data;
+
+	dprintk("%d %x %llx %llx\n", host_no, function, (unsigned long long) tag,
+		(unsigned long long) ev.k.tsk_mgmt_req.mid);
+
+	return send_event_rsp(TGT_KEVENT_TSK_MGMT_REQ, &ev, GFP_KERNEL, tgtd_pid);
 }
 
 static int event_recv_msg(struct sk_buff *skb, struct nlmsghdr *nlh)
@@ -124,19 +139,22 @@ static int event_recv_msg(struct sk_buff
 		nlh->nlmsg_pid, current->pid);
 
 	switch (nlh->nlmsg_type) {
-	case TGT_UEVENT_TGTD_BIND:
+	case TGT_UEVENT_REQ:
 		tgtd_pid = NETLINK_CREDS(skb)->pid;
 		break;
-	case TGT_UEVENT_CMD_RES:
+	case TGT_UEVENT_CMD_RSP:
 		/* TODO: handle multiple cmds in one event */
-		err = scsi_tgt_kspace_exec(ev->u.cmd_res.host_no,
-					   ev->u.cmd_res.cid,
-					   ev->u.cmd_res.result,
-					   ev->u.cmd_res.len,
-					   ev->u.cmd_res.offset,
-					   ev->u.cmd_res.uaddr,
-					   ev->u.cmd_res.rw,
-					   ev->u.cmd_res.try_map);
+		err = scsi_tgt_kspace_exec(ev->u.cmd_rsp.host_no,
+					   ev->u.cmd_rsp.cid,
+					   ev->u.cmd_rsp.result,
+					   ev->u.cmd_rsp.len,
+					   ev->u.cmd_rsp.uaddr,
+					   ev->u.cmd_rsp.rw);
+		break;
+	case TGT_UEVENT_TSK_MGMT_RSP:
+		err = scsi_tgt_kspace_tsk_mgmt(ev->u.tsk_mgmt_rsp.host_no,
+					       ev->u.tsk_mgmt_rsp.mid,
+					       ev->u.tsk_mgmt_rsp.result);
 		break;
 	default:
 		eprintk("unknown type %d\n", nlh->nlmsg_type);
@@ -151,6 +169,7 @@ static int event_recv_skb(struct sk_buff
 	int err;
 	uint32_t rlen;
 	struct nlmsghdr	*nlh;
+	struct tgt_event ev;
 
 	while (skb->len >= NLMSG_SPACE(0)) {
 		nlh = (struct nlmsghdr *) skb->data;
@@ -166,12 +185,14 @@ static int event_recv_skb(struct sk_buff
 		 * TODO for passthru commands the lower level should
 		 * probably handle the result or we should modify this
 		 */
-		if (nlh->nlmsg_type != TGT_UEVENT_CMD_RES) {
-			struct tgt_event ev;
-
+		switch (nlh->nlmsg_type) {
+		case TGT_UEVENT_CMD_RSP:
+		case TGT_UEVENT_TSK_MGMT_RSP:
+			break;
+		default:
 			memset(&ev, 0, sizeof(ev));
-			ev.k.event_res.err = err;
-			send_event_res(TGT_KEVENT_RESPONSE, &ev, NULL, 0,
+			ev.k.event_rsp.err = err;
+			send_event_rsp(TGT_KEVENT_RSP, &ev,
 				       GFP_KERNEL | __GFP_NOFAIL,
 					nlh->nlmsg_pid);
 		}
diff --git a/drivers/scsi/scsi_tgt_lib.c b/drivers/scsi/scsi_tgt_lib.c
index 8746236..5a98fc4 100644
--- a/drivers/scsi/scsi_tgt_lib.c
+++ b/drivers/scsi/scsi_tgt_lib.c
@@ -20,7 +20,7 @@
  * 02110-1301 USA
  */
 #include <linux/blkdev.h>
-#include <linux/elevator.h>
+#include <linux/hash.h>
 #include <linux/module.h>
 #include <linux/pagemap.h>
 #include <scsi/scsi.h>
@@ -46,6 +46,25 @@ struct scsi_tgt_cmd {
 	struct bio_list xfer_done_list;
 	struct bio_list xfer_list;
 	struct scsi_lun *lun;
+
+	struct list_head hash_list;
+	struct request *rq;
+	u64 tag;
+};
+
+#define TGT_HASH_ORDER	4
+#define cmd_hashfn(cid)	hash_long((cid), TGT_HASH_ORDER)
+
+struct scsi_tgt_queuedata {
+	struct Scsi_Host *shost;
+	struct list_head cmd_hash[1 << TGT_HASH_ORDER];
+	spinlock_t cmd_hash_lock;
+
+	struct work_struct uspace_send_work;
+
+	spinlock_t cmd_req_lock;
+	struct mutex cmd_req_mutex;
+	struct list_head cmd_req;
 };
 
 static void scsi_unmap_user_pages(struct scsi_tgt_cmd *tcmd)
@@ -68,9 +87,16 @@ static void scsi_tgt_cmd_destroy(void *d
 {
 	struct scsi_cmnd *cmd = data;
 	struct scsi_tgt_cmd *tcmd = cmd->request->end_io_data;
+	struct scsi_tgt_queuedata *qdata = cmd->request->q->queuedata;
+	unsigned long flags;
 
 	dprintk("cmd %p %d %lu\n", cmd, cmd->sc_data_direction,
 		rq_data_dir(cmd->request));
+
+	spin_lock_irqsave(&qdata->cmd_hash_lock, flags);
+	list_del(&tcmd->hash_list);
+	spin_unlock_irqrestore(&qdata->cmd_hash_lock, flags);
+
 	/*
 	 * We must set rq->flags here because bio_map_user and
 	 * blk_rq_bio_prep ruined ti.
@@ -81,62 +107,71 @@ static void scsi_tgt_cmd_destroy(void *d
 		cmd->request->flags &= ~1UL;
 
 	scsi_unmap_user_pages(tcmd);
-	scsi_tgt_uspace_send_status(cmd, GFP_KERNEL);
 	kmem_cache_free(scsi_tgt_cmd_cache, tcmd);
 	scsi_host_put_command(scsi_tgt_cmd_to_host(cmd), cmd);
 }
 
 static void init_scsi_tgt_cmd(struct request *rq, struct scsi_tgt_cmd *tcmd)
 {
-	tcmd->lun = rq->end_io_data;
-	bio_list_init(&tcmd->xfer_list);
-	bio_list_init(&tcmd->xfer_done_list);
+	struct scsi_tgt_queuedata *qdata = rq->q->queuedata;
+	unsigned long flags;
+	struct list_head *head;
+	static u32 tag = 0;
+
+	spin_lock_irqsave(&qdata->cmd_hash_lock, flags);
+	rq->tag = tag++;
+	head = &qdata->cmd_hash[cmd_hashfn(rq->tag)];
+	list_add(&tcmd->hash_list, head);
+	spin_unlock_irqrestore(&qdata->cmd_hash_lock, flags);
 }
 
-static int scsi_uspace_prep_fn(struct request_queue *q, struct request *rq)
-{
-	struct scsi_tgt_cmd *tcmd;
-
-	tcmd = kmem_cache_alloc(scsi_tgt_cmd_cache, GFP_ATOMIC);
-	if (!tcmd)
-		return BLKPREP_DEFER;
-
-	init_scsi_tgt_cmd(rq, tcmd);
-	rq->end_io_data = tcmd;
-	rq->flags |= REQ_DONTPREP;
-	return BLKPREP_OK;
-}
-
-static void scsi_uspace_request_fn(struct request_queue *q)
+static void scsi_tgt_uspace_send_fn(void *data)
 {
+	struct request_queue *q = data;
+	struct scsi_tgt_queuedata *qdata = q->queuedata;
 	struct request *rq;
 	struct scsi_cmnd *cmd;
 	struct scsi_tgt_cmd *tcmd;
+	unsigned long flags;
+	int err;
 
-	/*
-	 * TODO: just send everthing in the queue to userspace in
-	 * one vector instead of multiple calls
-	 */
-	while ((rq = elv_next_request(q)) != NULL) {
-		cmd = rq->special;
-		tcmd = rq->end_io_data;
+retry:
+	err = 0;
+	if (list_empty(&qdata->cmd_req))
+		return;
 
-		/* the completion code kicks us in case we hit this */
-		if (blk_queue_start_tag(q, rq))
-			break;
+	mutex_lock(&qdata->cmd_req_mutex);
 
-		spin_unlock_irq(q->queue_lock);
-		if (scsi_tgt_uspace_send(cmd, tcmd->lun, GFP_ATOMIC) < 0)
-			goto requeue;
-		spin_lock_irq(q->queue_lock);
+	spin_lock_irqsave(&qdata->cmd_req_lock, flags);
+	if (list_empty(&qdata->cmd_req)) {
+		spin_unlock_irqrestore(&qdata->cmd_req_lock, flags);
+		mutex_unlock(&qdata->cmd_req_mutex);
+		goto out;
+	}
+	rq = list_entry_rq(qdata->cmd_req.next);
+	list_del_init(&rq->queuelist);
+	spin_unlock_irqrestore(&qdata->cmd_req_lock, flags);
+
+	tcmd = rq->end_io_data;
+	init_scsi_tgt_cmd(rq, tcmd);
+	cmd = rq->special;
+	err = scsi_tgt_uspace_send(cmd, tcmd->lun, tcmd->tag, GFP_ATOMIC);
+	if (err < 0) {
+		eprintk("failed to send: %p %d\n", cmd, err);
+
+		spin_lock_irqsave(&qdata->cmd_req_lock, flags);
+		list_add(&rq->queuelist, &qdata->cmd_req);
+		spin_unlock_irqrestore(&qdata->cmd_req_lock, flags);
 	}
 
-	return;
-requeue:
-	spin_lock_irq(q->queue_lock);
-	/* need to track cnts and plug */
-	blk_requeue_request(q, rq);
-	spin_lock_irq(q->queue_lock);
+	mutex_unlock(&qdata->cmd_req_mutex);
+out:
+	/* TODO: proper error handling */
+	if (err < 0)
+		queue_delayed_work(scsi_tgtd, &qdata->uspace_send_work,
+				   HZ / 10);
+	else
+		goto retry;
 }
 
 /**
@@ -150,13 +185,13 @@ int scsi_tgt_alloc_queue(struct Scsi_Hos
 {
 	struct scsi_tgt_queuedata *queuedata;
 	struct request_queue *q;
-	int err;
+	int err, i;
 
 	/*
 	 * Do we need to send a netlink event or should uspace
 	 * just respond to the hotplug event?
 	 */
-	q = __scsi_alloc_queue(shost, scsi_uspace_request_fn);
+	q = __scsi_alloc_queue(shost, NULL);
 	if (!q)
 		return -ENOMEM;
 
@@ -168,19 +203,12 @@ int scsi_tgt_alloc_queue(struct Scsi_Hos
 	queuedata->shost = shost;
 	q->queuedata = queuedata;
 
-	elevator_exit(q->elevator);
-	err = elevator_init(q, "noop");
-	if (err)
-		goto free_data;
-
-	blk_queue_prep_rq(q, scsi_uspace_prep_fn);
 	/*
 	 * this is a silly hack. We should probably just queue as many
 	 * command as is recvd to userspace. uspace can then make
 	 * sure we do not overload the HBA
 	 */
 	q->nr_requests = shost->hostt->can_queue;
-	blk_queue_init_tags(q, shost->hostt->can_queue, NULL);
 	/*
 	 * We currently only support software LLDs so this does
 	 * not matter for now. Do we need this for the cards we support?
@@ -189,10 +217,17 @@ int scsi_tgt_alloc_queue(struct Scsi_Hos
 	blk_queue_dma_alignment(q, 0);
 	shost->uspace_req_q = q;
 
+	for (i = 0; i < ARRAY_SIZE(queuedata->cmd_hash); i++)
+		INIT_LIST_HEAD(&queuedata->cmd_hash[i]);
+	spin_lock_init(&queuedata->cmd_hash_lock);
+
+	INIT_LIST_HEAD(&queuedata->cmd_req);
+	spin_lock_init(&queuedata->cmd_req_lock);
+	INIT_WORK(&queuedata->uspace_send_work, scsi_tgt_uspace_send_fn, q);
+	mutex_init(&queuedata->cmd_req_mutex);
+
 	return 0;
 
-free_data:
-	kfree(queuedata);
 cleanup_queue:
 	blk_cleanup_queue(q);
 	return err;
@@ -212,17 +247,35 @@ EXPORT_SYMBOL_GPL(scsi_tgt_cmd_to_host);
  * @scsilun:	scsi lun
  * @noblock:	set to nonzero if the command should be queued
  **/
-void scsi_tgt_queue_command(struct scsi_cmnd *cmd, struct scsi_lun *scsilun,
-			    int noblock)
+int scsi_tgt_queue_command(struct scsi_cmnd *cmd, struct scsi_lun *scsilun,
+			   u64 tag)
 {
+	struct request_queue *q = cmd->request->q;
+	struct scsi_tgt_queuedata *qdata = q->queuedata;
+	unsigned long flags;
+	struct scsi_tgt_cmd *tcmd;
+
 	/*
-	 * For now this just calls the request_fn from this context.
-	 * For HW llds though we do not want to execute from here so
-	 * the elevator code needs something like a REQ_TGT_CMD or
-	 * REQ_MSG_DONT_UNPLUG_IMMED_BECUASE_WE_WILL_HANDLE_IT
+	 * It would be better to allocate scsi_tgt_cmd structure in
+	 * scsi_host_get_command and not to fail due to OOM.
 	 */
-	cmd->request->end_io_data = scsilun;
-	elv_add_request(cmd->request->q, cmd->request, ELEVATOR_INSERT_BACK, 1);
+	tcmd = kmem_cache_alloc(scsi_tgt_cmd_cache, GFP_ATOMIC);
+	if (!tcmd)
+		return -ENOMEM;
+	cmd->request->end_io_data = tcmd;
+
+	bio_list_init(&tcmd->xfer_list);
+	bio_list_init(&tcmd->xfer_done_list);
+	tcmd->lun = scsilun;
+	tcmd->tag = tag;
+	tcmd->rq = cmd->request;
+
+	spin_lock_irqsave(&qdata->cmd_req_lock, flags);
+	list_add_tail(&cmd->request->queuelist, &qdata->cmd_req);
+	spin_unlock_irqrestore(&qdata->cmd_req_lock, flags);
+
+	queue_work(scsi_tgtd, &qdata->uspace_send_work);
+	return 0;
 }
 EXPORT_SYMBOL_GPL(scsi_tgt_queue_command);
 
@@ -236,12 +289,7 @@ static void scsi_tgt_cmd_done(struct scs
 
 	dprintk("cmd %p %lu\n", cmd, rq_data_dir(cmd->request));
 
-	/* don't we have to call this if result is set or not */
-	if (cmd->result) {
-		scsi_tgt_uspace_send_status(cmd, GFP_ATOMIC);
-		return;
-	}
-
+	scsi_tgt_uspace_send_status(cmd, GFP_ATOMIC);
 	INIT_WORK(&tcmd->work, scsi_tgt_cmd_destroy, cmd);
 	queue_work(scsi_tgtd, &tcmd->work);
 }
@@ -315,7 +363,7 @@ static int scsi_map_user_pages(struct sc
 
 	while (len > 0) {
 		dprintk("%lx %u\n", (unsigned long) uaddr, len);
-		bio = bio_map_user(q, NULL, (unsigned long) uaddr, len, rw, 1);
+		bio = bio_map_user(q, NULL, (unsigned long) uaddr, len, rw);
 		if (IS_ERR(bio)) {
 			err = PTR_ERR(bio);
 			dprintk("fail to map %lx %u %d %x\n",
@@ -438,16 +486,49 @@ static int scsi_tgt_copy_sense(struct sc
 	return 0;
 }
 
-int scsi_tgt_kspace_exec(int host_no, u32 cid, int result, u32 len, u64 offset,
-			 unsigned long uaddr, u8 rw, u8 try_map)
+static int scsi_tgt_abort_cmd(struct Scsi_Host *host, struct scsi_cmnd *cmd)
+{
+	int err;
+
+	err = host->hostt->eh_abort_handler(cmd);
+	if (err)
+		eprintk("fail to abort %p\n", cmd);
+
+	scsi_tgt_cmd_destroy(cmd);
+	return err;
+}
+
+static struct request *tgt_cmd_hash_lookup(struct request_queue *q, u32 cid)
+{
+	struct scsi_tgt_queuedata *qdata = q->queuedata;
+	struct request *rq = NULL;
+	struct list_head *head;
+	struct scsi_tgt_cmd *tcmd;
+	unsigned long flags;
+
+	head = &qdata->cmd_hash[cmd_hashfn(cid)];
+	spin_lock_irqsave(&qdata->cmd_hash_lock, flags);
+	list_for_each_entry(tcmd, head, hash_list) {
+		if (tcmd->rq->tag == cid) {
+			rq = tcmd->rq;
+			break;
+		}
+	}
+	spin_unlock_irqrestore(&qdata->cmd_hash_lock, flags);
+
+	return rq;
+}
+
+int scsi_tgt_kspace_exec(int host_no, u32 cid, int result, u32 len,
+			 unsigned long uaddr, u8 rw)
 {
 	struct Scsi_Host *shost;
 	struct scsi_cmnd *cmd;
 	struct request *rq;
 	int err = 0;
 
-	dprintk("%d %u %d %u %llu %lx %u %u\n", host_no, cid, result,
-		len, (unsigned long long) offset, uaddr, rw, try_map);
+	dprintk("%d %u %d %u %lx %u\n", host_no, cid, result,
+		len, uaddr, rw);
 
 	/* TODO: replace with a O(1) alg */
 	shost = scsi_host_lookup(host_no);
@@ -456,7 +537,7 @@ int scsi_tgt_kspace_exec(int host_no, u3
 		return -EINVAL;
 	}
 
-	rq = blk_queue_find_tag(shost->uspace_req_q, cid);
+	rq = tgt_cmd_hash_lookup(shost->uspace_req_q, cid);
 	if (!rq) {
 		printk(KERN_ERR "Could not find cid %u\n", cid);
 		err = -EINVAL;
@@ -467,6 +548,10 @@ int scsi_tgt_kspace_exec(int host_no, u3
 	dprintk("cmd %p result %d len %d bufflen %u %lu %x\n", cmd,
 		result, len, cmd->request_bufflen, rq_data_dir(rq), cmd->cmnd[0]);
 
+	if (result == TASK_ABORTED) {
+		scsi_tgt_abort_cmd(shost, cmd);
+		goto done;
+	}
 	/*
 	 * store the userspace values here, the working values are
 	 * in the request_* values
@@ -507,6 +592,38 @@ done:
 	return err;
 }
 
+int scsi_tgt_tsk_mgmt_request(struct Scsi_Host *shost, int function, u64 tag,
+			      struct scsi_lun *scsilun, void *data)
+{
+	int err;
+
+	/* TODO: need to retry if this fails. */
+	err = scsi_tgt_uspace_send_tsk_mgmt(shost->host_no, function,
+					    tag, scsilun, data);
+	if (err < 0)
+		eprintk("The task management request lost!\n");
+	return err;
+}
+EXPORT_SYMBOL_GPL(scsi_tgt_tsk_mgmt_request);
+
+int scsi_tgt_kspace_tsk_mgmt(int host_no, u64 mid, int result)
+{
+	struct Scsi_Host *shost;
+	int err;
+
+	dprintk("%d %d %llx\n", host_no, result, (unsigned long long) mid);
+
+	shost = scsi_host_lookup(host_no);
+	if (IS_ERR(shost)) {
+		printk(KERN_ERR "Could not find host no %d\n", host_no);
+		return -EINVAL;
+	}
+	err = shost->hostt->tsk_mgmt_response(mid, result);
+	scsi_host_put(shost);
+
+	return err;
+}
+
 static int __init scsi_tgt_init(void)
 {
 	int err;
diff --git a/drivers/scsi/scsi_tgt_priv.h b/drivers/scsi/scsi_tgt_priv.h
index 4236e50..77a1d06 100644
--- a/drivers/scsi/scsi_tgt_priv.h
+++ b/drivers/scsi/scsi_tgt_priv.h
@@ -4,22 +4,21 @@ struct Scsi_Host;
 struct task_struct;
 
 /* tmp - will replace with SCSI logging stuff */
-#define dprintk(fmt, args...)					\
+#define eprintk(fmt, args...)					\
 do {								\
 	printk("%s(%d) " fmt, __FUNCTION__, __LINE__, ##args);	\
 } while (0)
 
-#define eprintk dprintk
-
-struct scsi_tgt_queuedata {
-	struct Scsi_Host *shost;
-};
+#define dprintk eprintk
 
 extern void scsi_tgt_if_exit(void);
 extern int scsi_tgt_if_init(void);
 
-extern int scsi_tgt_uspace_send(struct scsi_cmnd *cmd, struct scsi_lun *lun, gfp_t flags);
+extern int scsi_tgt_uspace_send(struct scsi_cmnd *cmd, struct scsi_lun *lun,
+				u64 tag, gfp_t flags);
 extern int scsi_tgt_uspace_send_status(struct scsi_cmnd *cmd, gfp_t flags);
 extern int scsi_tgt_kspace_exec(int host_no, u32 cid, int result, u32 len,
-				u64 offset, unsigned long uaddr, u8 rw,
-				u8 try_map);
+				unsigned long uaddr, u8 rw);
+extern int scsi_tgt_uspace_send_tsk_mgmt(int host_no, int function, u64 tag,
+					 struct scsi_lun *scsilun, void *data);
+extern int scsi_tgt_kspace_tsk_mgmt(int host_no, u64 mid, int result);
diff --git a/fs/bio.c b/fs/bio.c
index 3e940c9..f51a873 100644
--- a/fs/bio.c
+++ b/fs/bio.c
@@ -718,21 +718,19 @@ static struct bio *__bio_map_user_iov(re
  *	@uaddr: start of user address
  *	@len: length in bytes
  *	@write_to_vm: bool indicating writing to pages or not
- *	@support_partial: support partial mappings
  *
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
 struct bio *bio_map_user(request_queue_t *q, struct block_device *bdev,
-			 unsigned long uaddr, unsigned int len, int write_to_vm,
-			 int support_partial)
+			 unsigned long uaddr, unsigned int len, int write_to_vm)
 {
 	struct sg_iovec iov;
 
 	iov.iov_base = (void __user *)uaddr;
 	iov.iov_len = len;
 
-	return bio_map_user_iov(q, bdev, &iov, 1, write_to_vm, support_partial);
+	return bio_map_user_iov(q, bdev, &iov, 1, write_to_vm);
 }
 
 /**
@@ -742,17 +740,15 @@ struct bio *bio_map_user(request_queue_t
  *	@iov:	the iovec.
  *	@iov_count: number of elements in the iovec
  *	@write_to_vm: bool indicating writing to pages or not
- *	@support_partial: support partial mappings
  *
  *	Map the user space address into a bio suitable for io to a block
  *	device. Returns an error pointer in case of error.
  */
 struct bio *bio_map_user_iov(request_queue_t *q, struct block_device *bdev,
 			     struct sg_iovec *iov, int iov_count,
-			     int write_to_vm, int support_partial)
+			     int write_to_vm)
 {
 	struct bio *bio;
-	int len = 0, i;
 
 	bio = __bio_map_user_iov(q, bdev, iov, iov_count, write_to_vm);
 
@@ -767,18 +763,7 @@ struct bio *bio_map_user_iov(request_que
 	 */
 	bio_get(bio);
 
-	for (i = 0; i < iov_count; i++)
-		len += iov[i].iov_len;
-
-	if (bio->bi_size == len || support_partial)
-		return bio;
-
-	/*
-	 * don't support partial mappings
-	 */
-	bio_endio(bio, bio->bi_size, 0);
-	bio_unmap_user(bio);
-	return ERR_PTR(-EINVAL);
+	return bio;
 }
 
 static void __bio_unmap_user(struct bio *bio)
diff --git a/include/linux/bio.h b/include/linux/bio.h
index fc0906c..b60ffe3 100644
--- a/include/linux/bio.h
+++ b/include/linux/bio.h
@@ -295,13 +295,12 @@ extern int bio_add_page(struct bio *, st
 extern int bio_add_pc_page(struct request_queue *, struct bio *, struct page *,
 			   unsigned int, unsigned int);
 extern int bio_get_nr_vecs(struct block_device *);
-extern int __bio_get_nr_vecs(struct request_queue *);
 extern struct bio *bio_map_user(struct request_queue *, struct block_device *,
-				unsigned long, unsigned int, int, int);
+				unsigned long, unsigned int, int);
 struct sg_iovec;
 extern struct bio *bio_map_user_iov(struct request_queue *,
 				    struct block_device *,
-				    struct sg_iovec *, int, int, int);
+				    struct sg_iovec *, int, int);
 extern void bio_unmap_user(struct bio *);
 extern struct bio *bio_map_kern(struct request_queue *, void *, unsigned int,
 				gfp_t);
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 860e7a4..619ef1d 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -611,7 +611,8 @@ extern void blk_queue_activity_fn(reques
 extern int blk_rq_map_user(request_queue_t *, struct request *, void __user *, unsigned int);
 extern int blk_rq_unmap_user(struct bio *, unsigned int);
 extern int blk_rq_map_kern(request_queue_t *, struct request *, void *, unsigned int, gfp_t);
-extern int blk_rq_map_user_iov(request_queue_t *, struct request *, struct sg_iovec *, int);
+extern int blk_rq_map_user_iov(request_queue_t *, struct request *,
+			       struct sg_iovec *, int, unsigned int);
 extern int blk_execute_rq(request_queue_t *, struct gendisk *,
 			  struct request *, int);
 extern void blk_execute_rq_nowait(request_queue_t *, struct gendisk *,
diff --git a/include/scsi/scsi_host.h b/include/scsi/scsi_host.h
index 8b799db..eca5721 100644
--- a/include/scsi/scsi_host.h
+++ b/include/scsi/scsi_host.h
@@ -153,6 +153,9 @@ struct scsi_host_template {
 	int (* transfer_data)(struct scsi_cmnd *,
 			      void (*done)(struct scsi_cmnd *));
 
+	/* Used as callback for the completion of task management request. */
+	int (* tsk_mgmt_response)(u64 mid, int result);
+
 	/*
 	 * This is an error handling strategy routine.  You don't need to
 	 * define one of these if you don't want to - there is a default
diff --git a/include/scsi/scsi_tgt.h b/include/scsi/scsi_tgt.h
index 91ad6bc..2d65be7 100644
--- a/include/scsi/scsi_tgt.h
+++ b/include/scsi/scsi_tgt.h
@@ -6,6 +6,8 @@ struct Scsi_Host;
 struct scsi_cmnd;
 struct scsi_lun;
 
-extern struct Scsi_Host *scsi_tgt_cmd_to_host(struct scsi_cmnd *cmd);
+extern struct Scsi_Host *scsi_tgt_cmd_to_host(struct scsi_cmnd *);
 extern int scsi_tgt_alloc_queue(struct Scsi_Host *);
-extern void scsi_tgt_queue_command(struct scsi_cmnd *, struct scsi_lun *, int);
+extern int scsi_tgt_queue_command(struct scsi_cmnd *, struct scsi_lun *, u64);
+extern int scsi_tgt_tsk_mgmt_request(struct Scsi_Host *, int, u64, struct scsi_lun *,
+				     void *);
diff --git a/include/scsi/scsi_tgt_if.h b/include/scsi/scsi_tgt_if.h
index da3a808..63b2e3a 100644
--- a/include/scsi/scsi_tgt_if.h
+++ b/include/scsi/scsi_tgt_if.h
@@ -24,65 +24,66 @@
 
 enum tgt_event_type {
 	/* user -> kernel */
-	TGT_UEVENT_TGTD_BIND,
-	TGT_UEVENT_TARGET_SETUP,
-	TGT_UEVENT_CMD_RES,
+	TGT_UEVENT_REQ,
+	TGT_UEVENT_CMD_RSP,
+	TGT_UEVENT_TSK_MGMT_RSP,
 
 	/* kernel -> user */
-	TGT_KEVENT_RESPONSE,
+	TGT_KEVENT_RSP,
 	TGT_KEVENT_CMD_REQ,
 	TGT_KEVENT_CMD_DONE,
+	TGT_KEVENT_TSK_MGMT_REQ,
 };
 
 struct tgt_event {
 	/* user-> kernel */
 	union {
 		struct {
-			int pk_fd;
-		} tgtd_bind;
+			int type;
+			int host_no;
+		} event_req;
 		struct {
 			int host_no;
 			uint32_t cid;
 			uint32_t len;
 			int result;
 			uint64_t uaddr;
-			uint64_t offset;
 			uint8_t rw;
-			uint8_t try_map;
-		} cmd_res;
+		} cmd_rsp;
+		struct {
+			int host_no;
+			uint64_t mid;
+			int result;
+		} tsk_mgmt_rsp;
 	} u;
 
 	/* kernel -> user */
 	union {
 		struct {
 			int err;
-		} event_res;
+		} event_rsp;
 		struct {
 			int host_no;
 			uint32_t cid;
 			uint32_t data_len;
-			uint64_t dev_id;
+			uint8_t scb[16];
+			uint8_t lun[8];
+			int attribute;
+			uint64_t tag;
 		} cmd_req;
 		struct {
 			int host_no;
 			uint32_t cid;
 			int result;
 		} cmd_done;
+		struct {
+			int host_no;
+			int function;
+			uint64_t tag;
+			uint8_t lun[8];
+			uint64_t mid;
+		} tsk_mgmt_req;
 	} k;
 
-	/*
-	 * I think a pointer is a unsigned long but this struct
-	 * gets passed around from the kernel to userspace and
-	 * back again so to handle some ppc64 setups where userspace is
-	 * 32 bits but the kernel is 64 we do this odd thing
-	 */
-	uint64_t data[0];
-} __attribute__ ((aligned (sizeof(uint64_t))));
-
-struct tgt_cmd {
-	uint8_t scb[16];
-	uint8_t lun[8];
-	int tags;
 } __attribute__ ((aligned (sizeof(uint64_t))));
-
 #endif
